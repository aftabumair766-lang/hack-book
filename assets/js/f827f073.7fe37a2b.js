"use strict";(self.webpackChunkhack_book=self.webpackChunkhack_book||[]).push([[164],{1566:(e,n,i)=>{i.d(n,{A:()=>d});var r=i(2444),s=i(467),l=i(6540),a=i(9092);const t={translateContainer:"translateContainer_k3TY",buttonGroup:"buttonGroup_jUGV",translateButton:"translateButton_oCa0",icon:"icon_AqWM",sparkle:"sparkle_LzoX",spinner:"spinner_R0Ye",spin:"spin_pj56",languageDropdown:"languageDropdown_WJW9",dropdownHeader:"dropdownHeader_dQJJ",languageOption:"languageOption_wOrH",flag:"flag_zwrE",native:"native_Urju",english:"english_cu3c",translatedBanner:"translatedBanner_gzzJ",checkIcon:"checkIcon_KBHa",resetButton:"resetButton_Yazw",error:"error_Vs_S"};var o=i(4848),c="undefined"!=typeof window&&"localhost"!==window.location.hostname?"https://your-backend-url.com":"http://localhost:8000";function d(e){var n=e.chapterId,i=e.onTranslate,d=(0,a.A)(),h=d.isAuthenticated,p=d.token,u=(0,l.useState)(!1),x=u[0],m=u[1],g=(0,l.useState)(""),j=g[0],f=g[1],v=(0,l.useState)(!1),b=v[0],y=v[1],k=(0,l.useState)(!1),w=k[0],S=k[1],A=function(){var e=(0,s.A)((0,r.A)().m(function e(s){var l,a,t,o;return(0,r.A)().w(function(e){for(;;)switch(e.p=e.n){case 0:if(h&&p){e.n=1;break}return f("Please sign in to translate content"),e.a(2);case 1:return m(!0),f(""),S(!1),e.p=2,e.n=3,fetch(c+"/api/translate/chapter",{method:"POST",headers:{"Content-Type":"application/json",Authorization:"Bearer "+p},body:JSON.stringify({chapter_id:n,target_language:s})});case 3:if((l=e.v).ok){e.n=5;break}return e.n=4,l.json();case 4:throw a=e.v,new Error(a.detail||"Translation failed");case 5:return e.n=6,l.json();case 6:t=e.v,i&&i(t),y(!0),e.n=8;break;case 7:e.p=7,o=e.v,f(o.message),console.error("Translation error:",o);case 8:return e.p=8,m(!1),e.f(8);case 9:return e.a(2)}},e,null,[[2,7,8,9]])}));return function(n){return e.apply(this,arguments)}}();return h?(0,o.jsxs)("div",{className:t.translateContainer,children:[b?(0,o.jsxs)("div",{className:t.translatedBanner,children:[(0,o.jsx)("span",{className:t.checkIcon,children:"\u2713"}),"Content translated successfully!",(0,o.jsx)("button",{onClick:function(){y(!1),i&&i(null)},className:t.resetButton,children:"View Original (English)"})]}):(0,o.jsxs)("div",{className:t.buttonGroup,children:[(0,o.jsx)("button",{onClick:function(){return S(!w)},disabled:x,className:t.translateButton,children:x?(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)("span",{className:t.spinner}),"Translating..."]}):(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)("span",{className:t.icon,children:"\ud83c\udf0d"}),"Translate to Urdu / Other Languages"]})}),w&&!x&&(0,o.jsxs)("div",{className:t.languageDropdown,children:[(0,o.jsx)("div",{className:t.dropdownHeader,children:"Select Language:"}),[{code:"urdu",name:"Urdu",native:"\u0627\u0631\u062f\u0648",flag:"\ud83c\uddf5\ud83c\uddf0"},{code:"arabic",name:"Arabic",native:"\u0627\u0644\u0639\u0631\u0628\u064a\u0629",flag:"\ud83c\uddf8\ud83c\udde6"},{code:"spanish",name:"Spanish",native:"Espa\xf1ol",flag:"\ud83c\uddea\ud83c\uddf8"},{code:"french",name:"French",native:"Fran\xe7ais",flag:"\ud83c\uddeb\ud83c\uddf7"},{code:"german",name:"German",native:"Deutsch",flag:"\ud83c\udde9\ud83c\uddea"},{code:"hindi",name:"Hindi",native:"\u0939\u093f\u0928\u094d\u0926\u0940",flag:"\ud83c\uddee\ud83c\uddf3"},{code:"chinese",name:"Chinese",native:"\u4e2d\u6587",flag:"\ud83c\udde8\ud83c\uddf3"}].map(function(e){return(0,o.jsxs)("button",{onClick:function(){return A(e.code)},className:t.languageOption,children:[(0,o.jsx)("span",{className:t.flag,children:e.flag}),(0,o.jsx)("span",{className:t.native,children:e.native}),(0,o.jsxs)("span",{className:t.english,children:["(",e.name,")"]})]},e.code)})]})]}),j&&(0,o.jsx)("div",{className:t.error,children:j})]}):null}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>t});var r=i(6540);const s={},l=r.createContext(s);function a(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(l.Provider,{value:n},e.children)}},9264:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"chapter-02/chapter-02","title":"Chapter 2: Artificial Intelligence in Physical Systems","description":"Explore how AI enables perception, decision-making, and learning in humanoid robots","source":"@site/docs/chapter-02/index.md","sourceDirName":"chapter-02","slug":"/chapter-02/","permalink":"/hack-book/docs/chapter-02/","draft":false,"unlisted":false,"editUrl":"https://github.com/aftabumair766-lang/hack-book/tree/main/docs/chapter-02/index.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"chapter-02","title":"Chapter 2: Artificial Intelligence in Physical Systems","sidebar_label":"Ch 2: AI in Physical Systems","sidebar_position":3,"description":"Explore how AI enables perception, decision-making, and learning in humanoid robots","keywords":["computer vision","machine learning","reinforcement learning","sensor processing","embodied AI"]},"sidebar":"coursebookSidebar","previous":{"title":"Chapter 1 Exercises","permalink":"/hack-book/docs/chapter-01/exercises"},"next":{"title":"Ch 3: Control Systems","permalink":"/hack-book/docs/chapter-03/"}}');var s=i(4848),l=i(8453),a=i(1566);const t={id:"chapter-02",title:"Chapter 2: Artificial Intelligence in Physical Systems",sidebar_label:"Ch 2: AI in Physical Systems",sidebar_position:3,description:"Explore how AI enables perception, decision-making, and learning in humanoid robots",keywords:["computer vision","machine learning","reinforcement learning","sensor processing","embodied AI"]},o="Chapter 2: Artificial Intelligence in Physical Systems",c={},d=[{value:"Introduction",id:"introduction",level:2},{value:"What You&#39;ll Learn",id:"what-youll-learn",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Key Topics",id:"key-topics",level:2},{value:"2.1 Computer Vision for Robotics",id:"21-computer-vision-for-robotics",level:3},{value:"2.2 Machine Learning for Sensor Processing",id:"22-machine-learning-for-sensor-processing",level:3},{value:"2.3 Reinforcement Learning for Control",id:"23-reinforcement-learning-for-control",level:3},{value:"2.4 Real-Time Decision Making",id:"24-real-time-decision-making",level:3},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Case Studies",id:"case-studies",level:2},{value:"Tesla Optimus",id:"tesla-optimus",level:3},{value:"Boston Dynamics Atlas",id:"boston-dynamics-atlas",level:3},{value:"Everyday Robots (Google)",id:"everyday-robots-google",level:3},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Exercise 2.1: Object Detection with YOLO",id:"exercise-21-object-detection-with-yolo",level:3},{value:"Exercise 2.2: Sensor Fusion with Kalman Filter",id:"exercise-22-sensor-fusion-with-kalman-filter",level:3},{value:"Exercise 2.3: Simple RL Agent",id:"exercise-23-simple-rl-agent",level:3},{value:"Exercise 2.4: Path Planning",id:"exercise-24-path-planning",level:3},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.A,{chapterId:"chapter-02"}),"\n",(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-2-artificial-intelligence-in-physical-systems",children:"Chapter 2: Artificial Intelligence in Physical Systems"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Physical AI combines artificial intelligence with robotic systems to enable robots to perceive, reason, and act in the real world. Unlike pure software AI, physical AI must handle uncertainty, sensor noise, and real-time constraints."}),"\n",(0,s.jsx)(n.h3,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Computer vision for robot perception"}),"\n",(0,s.jsx)(n.li,{children:"Machine learning for pattern recognition"}),"\n",(0,s.jsx)(n.li,{children:"Reinforcement learning for robot control"}),"\n",(0,s.jsx)(n.li,{children:"Sensor data processing and fusion"}),"\n",(0,s.jsx)(n.li,{children:"Real-time decision-making systems"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Chapter 1: Foundations of Humanoid Robotics"}),"\n",(0,s.jsx)(n.li,{children:"Basic Python programming"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of neural networks (helpful but not required)"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"key-topics",children:"Key Topics"}),"\n",(0,s.jsx)(n.h3,{id:"21-computer-vision-for-robotics",children:"2.1 Computer Vision for Robotics"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Object Detection and Recognition"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Convolutional Neural Networks (CNNs)"}),"\n",(0,s.jsx)(n.li,{children:"YOLO, R-CNN, and modern detection architectures"}),"\n",(0,s.jsx)(n.li,{children:"Real-time processing requirements"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Depth Estimation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Stereo vision algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Monocular depth estimation with deep learning"}),"\n",(0,s.jsx)(n.li,{children:"Applications: Navigation, obstacle avoidance, grasping"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Pose Estimation"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Human pose detection for interaction"}),"\n",(0,s.jsx)(n.li,{children:"Object pose estimation for manipulation"}),"\n",(0,s.jsx)(n.li,{children:"6DOF pose from RGB-D data"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"22-machine-learning-for-sensor-processing",children:"2.2 Machine Learning for Sensor Processing"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Sensor Fusion with ML"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Kalman filters for state estimation"}),"\n",(0,s.jsx)(n.li,{children:"Deep learning for multi-modal fusion"}),"\n",(0,s.jsx)(n.li,{children:"IMU + Vision integration for robust perception"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Time Series Analysis"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"LSTM networks for sequential sensor data"}),"\n",(0,s.jsx)(n.li,{children:"Predicting robot state from history"}),"\n",(0,s.jsx)(n.li,{children:"Anomaly detection for safety"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"23-reinforcement-learning-for-control",children:"2.3 Reinforcement Learning for Control"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Basics of RL"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Agent, environment, reward, policy"}),"\n",(0,s.jsx)(n.li,{children:"Q-learning and policy gradient methods"}),"\n",(0,s.jsx)(n.li,{children:"Sim-to-real transfer"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Applications in Humanoid Robotics"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Learning to walk (locomotion policies)"}),"\n",(0,s.jsx)(n.li,{children:"Learning to grasp (manipulation)"}),"\n",(0,s.jsx)(n.li,{children:"Learning from human demonstrations (imitation learning)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Challenges"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Sample efficiency (many trials needed)"}),"\n",(0,s.jsx)(n.li,{children:"Sim-to-real gap (simulation vs reality)"}),"\n",(0,s.jsx)(n.li,{children:"Safety during learning"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"24-real-time-decision-making",children:"2.4 Real-Time Decision Making"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Planning Algorithms"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Path planning (A*, RRT, PRM)"}),"\n",(0,s.jsx)(n.li,{children:"Motion planning with constraints"}),"\n",(0,s.jsx)(n.li,{children:"Reactive planning for dynamic environments"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Hierarchical Control"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"High-level planning (AI/ML)"}),"\n",(0,s.jsx)(n.li,{children:"Low-level control (feedback controllers)"}),"\n",(0,s.jsx)(n.li,{children:"Behavior trees for complex tasks"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Explain"})," how computer vision enables robot perception"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Describe"})," machine learning approaches for sensor processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Understand"})," reinforcement learning for robot control"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apply"})," basic ML models to robot sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Design"})," a hierarchical control architecture"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computer vision"})," enables robots to see and understand their environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CNNs"})," are the foundation for modern robot vision (detection, segmentation, pose)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor fusion"})," combines multiple sensors with ML for robust perception"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reinforcement learning"})," allows robots to learn behaviors through trial and error"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sim-to-real transfer"})," is critical for training policies safely"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time constraints"})," require efficient algorithms and hardware acceleration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hierarchical control"})," separates high-level planning from low-level execution"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"case-studies",children:"Case Studies"}),"\n",(0,s.jsx)(n.h3,{id:"tesla-optimus",children:"Tesla Optimus"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Vision-based perception (cameras, no LiDAR)"}),"\n",(0,s.jsx)(n.li,{children:"Neural network occupancy grids"}),"\n",(0,s.jsx)(n.li,{children:"End-to-end learning from human demonstrations"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"boston-dynamics-atlas",children:"Boston Dynamics Atlas"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Vision for terrain mapping"}),"\n",(0,s.jsx)(n.li,{children:"Learning-based locomotion controllers"}),"\n",(0,s.jsx)(n.li,{children:"Adaptive balance control"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"everyday-robots-google",children:"Everyday Robots (Google)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Large-scale fleet learning"}),"\n",(0,s.jsx)(n.li,{children:"Sim-to-real for manipulation tasks"}),"\n",(0,s.jsx)(n.li,{children:"Continual learning from experience"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-21-object-detection-with-yolo",children:"Exercise 2.1: Object Detection with YOLO"}),"\n",(0,s.jsx)(n.p,{children:"Implement real-time object detection using a pre-trained YOLO model on webcam input."}),"\n",(0,s.jsx)(n.h3,{id:"exercise-22-sensor-fusion-with-kalman-filter",children:"Exercise 2.2: Sensor Fusion with Kalman Filter"}),"\n",(0,s.jsx)(n.p,{children:"Fuse IMU and encoder data to estimate robot position."}),"\n",(0,s.jsx)(n.h3,{id:"exercise-23-simple-rl-agent",children:"Exercise 2.3: Simple RL Agent"}),"\n",(0,s.jsx)(n.p,{children:"Train a reinforcement learning agent to balance a pole (CartPole environment)."}),"\n",(0,s.jsx)(n.h3,{id:"exercise-24-path-planning",children:"Exercise 2.4: Path Planning"}),"\n",(0,s.jsx)(n.p,{children:"Implement A* algorithm for grid-based path planning with obstacles."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:['S. Levine et al., "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection," ',(0,s.jsx)(n.em,{children:"International Journal of Robotics Research"}),", 2018."]}),"\n",(0,s.jsxs)(n.li,{children:['OpenAI et al., "Learning Dexterous In-Hand Manipulation," ',(0,s.jsx)(n.em,{children:"International Journal of Robotics Research"}),", 2020."]}),"\n",(0,s.jsxs)(n.li,{children:['J. Tan et al., "Sim-to-Real: Learning Agile Locomotion For Quadruped Robots," ',(0,s.jsx)(n.em,{children:"RSS"}),", 2018."]}),"\n",(0,s.jsxs)(n.li,{children:["R. Sutton and A. Barto, ",(0,s.jsx)(n.em,{children:"Reinforcement Learning: An Introduction"}),", 2nd ed. MIT Press, 2018."]}),"\n",(0,s.jsxs)(n.li,{children:["I. Goodfellow et al., ",(0,s.jsx)(n.em,{children:"Deep Learning"}),". MIT Press, 2016."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Estimated Reading Time"}),": 55 minutes"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Previous"}),": ",(0,s.jsx)(n.a,{href:"../chapter-01/index.md",children:"Chapter 1: Foundations"})," | ",(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"/hack-book/docs/chapter-03/",children:"Chapter 3: Control Systems"})]})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);